{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\varva\\anaconda3\\lib\\site-packages (0.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\varva\\anaconda3\\lib\\site-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\varva\\anaconda3\\lib\\site-packages (from pandas) (2018.5)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\varva\\anaconda3\\lib\\site-packages (from pandas) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\varva\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 24.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\varva\\anaconda3\\lib\\site-packages (2.5.6)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\varva\\anaconda3\\lib\\site-packages (from openpyxl) (1.0.1)\n",
      "Requirement already satisfied: jdcal in c:\\users\\varva\\anaconda3\\lib\\site-packages (from openpyxl) (1.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 24.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rapidfuzz in c:\\users\\varva\\anaconda3\\lib\\site-packages (3.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 24.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rapidfuzz in c:\\users\\varva\\anaconda3\\lib\\site-packages (3.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 24.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\varva\\anaconda3\\lib\\site-packages (2.5.6)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\varva\\anaconda3\\lib\\site-packages (from openpyxl) (1.0.1)\n",
      "Requirement already satisfied: jdcal in c:\\users\\varva\\anaconda3\\lib\\site-packages (from openpyxl) (1.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 24.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in c:\\users\\varva\\anaconda3\\lib\\site-packages (2.1)\n",
      "Requirement already satisfied: decorator>=4.1.0 in c:\\users\\varva\\anaconda3\\lib\\site-packages (from networkx) (4.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 24.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "!pip install openpyxl --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "!pip install rapidfuzz --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "!pip install feedparser --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "import sys\n",
    "!{sys.executable} -m pip install rapidfuzz --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "!{sys.executable} -m pip install openpyxl --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "!{sys.executable} -m pip install networkx --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "import networkx as nx\n",
    "import re\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Brand names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varva\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"brands_for_cleaning.xlsx\"\n",
    "OUTPUT_FILE = \"brands_duplicates_clusters.xlsx\"\n",
    "SIMILARITY_THRESHOLD = 88\n",
    "\n",
    "df = pd.read_excel(INPUT_FILE)\n",
    "df[\"–ë—Ä–µ–Ω–¥\"] = df[\"–ë—Ä–µ–Ω–¥\"].astype(str)\n",
    "\n",
    "def clean_name(name):\n",
    "    name = str(name).lower()\n",
    "    for ch in [\"'\", \"‚Äô\", \"-\", \",\", \".\", '\"']:\n",
    "        name = name.replace(ch, \"\")\n",
    "    name = \" \".join(name.split())\n",
    "    return name\n",
    "\n",
    "df[\"–ù–∞–∑–≤–∞–Ω–∏–µ_–æ—á–∏—â–µ–Ω–Ω–æ–µ\"] = df[\"–ë—Ä–µ–Ω–¥\"].apply(clean_name)\n",
    "\n",
    "names = df[\"–ù–∞–∑–≤–∞–Ω–∏–µ_–æ—á–∏—â–µ–Ω–Ω–æ–µ\"].tolist()\n",
    "original_names = df[\"–ë—Ä–µ–Ω–¥\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making clusters to reveal the Brand names duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ì–æ—Ç–æ–≤–æ! –ö–ª–∞—Å—Ç–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ brands_duplicates_clusters.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ===== –≥—Ä–∞—Ñ =====\n",
    "G = nx.Graph()\n",
    "\n",
    "for i in range(len(names)):\n",
    "    G.add_node(original_names[i])\n",
    "    for j in range(i + 1, len(names)):\n",
    "        score = fuzz.token_sort_ratio(names[i], names[j])\n",
    "        if score >= SIMILARITY_THRESHOLD:\n",
    "            G.add_edge(original_names[i], original_names[j])\n",
    "\n",
    "# ===== –∫–ª–∞—Å—Ç–µ—Ä—ã =====\n",
    "clusters = list(nx.connected_components(G))\n",
    "\n",
    "cluster_list = []\n",
    "for idx, cluster in enumerate(clusters, start=1):\n",
    "    cluster_list.append({\n",
    "        \"–ö–ª–∞—Å—Ç–µ—Ä\": idx,\n",
    "        \"–ö–æ–ª-–≤–æ –±—Ä–µ–Ω–¥–æ–≤\": len(cluster),\n",
    "        \"–ë—Ä–µ–Ω–¥—ã\": \", \".join(str(b) for b in cluster)\n",
    "    })\n",
    "\n",
    "df_clusters = pd.DataFrame(cluster_list)\n",
    "\n",
    "# –°–æ—Ä—Ç–∏—Ä—É–µ–º\n",
    "df_clusters = df_clusters.sort_values(\n",
    "    by=\"–ö–æ–ª-–≤–æ –±—Ä–µ–Ω–¥–æ–≤\",\n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "df_clusters.to_excel(OUTPUT_FILE, index=False)\n",
    "print(f\"–ì–æ—Ç–æ–≤–æ! –ö–ª–∞—Å—Ç–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varva\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ì–æ—Ç–æ–≤–æ! –ë–∞–∑–∞ –æ—á–∏—â–µ–Ω–∞ –∏ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å–¥–µ–ª–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞ üéâ\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"data (41).xlsx\"\n",
    "OUTPUT_FILE = \"brands_cleaned.xlsx\"\n",
    "\n",
    "df = pd.read_excel(INPUT_FILE)\n",
    "df = df.loc[df[\"–ù–∞–∑–≤–∞–Ω–∏–µ —Å–¥–µ–ª–∫–∏\"].notna()]\n",
    "\n",
    "# —É–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã \"–ê–≤—Ç–æ—Å–¥–µ–ª–∫–∞: \" –∏ \"Autolead: \"\n",
    "df[\"–ù–∞–∑–≤–∞–Ω–∏–µ —Å–¥–µ–ª–∫–∏\"] = df[\"–ù–∞–∑–≤–∞–Ω–∏–µ —Å–¥–µ–ª–∫–∏\"].str.replace(r\"^(–ê–≤—Ç–æ—Å–¥–µ–ª–∫–∞: |Autolead: )\", \"\", regex=True, case=False)\n",
    "\n",
    "# —É–¥–∞–ª—è–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å \"–¢–µ—Å—Ç\"\n",
    "df = df.loc[~df[\"–ù–∞–∑–≤–∞–Ω–∏–µ —Å–¥–µ–ª–∫–∏\"].str.contains(r\"^—Ç–µ—Å—Ç\", case=False, regex=True)]\n",
    "\n",
    "\n",
    "# ===== –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö –∫–æ–ª–æ–Ω–æ–∫ =====\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# –î–µ–ª–∞–µ–º datetime\n",
    "df[\"–î–∞—Ç–∞ –∑–∞–∫—Ä—ã—Ç–∏—è\"] = pd.to_datetime(df[\"–î–∞—Ç–∞ –∑–∞–∫—Ä—ã—Ç–∏—è\"], errors=\"coerce\")\n",
    "\n",
    "# –ó–∞–º–µ–Ω—è–µ–º –∑–∞–≥–ª—É—à–∫—É 9999-12-31 –Ω–∞ –ø—É—Å—Ç–æ–µ\n",
    "df.loc[df[\"–î–∞—Ç–∞ –∑–∞–∫—Ä—ã—Ç–∏—è\"] == pd.Timestamp(\"9999-12-31\"), \"–î–∞—Ç–∞ –∑–∞–∫—Ä—ã—Ç–∏—è\"] = pd.NaT\n",
    "\n",
    "\n",
    "# ===== –û–±—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ =====\n",
    "def is_garbage(text):\n",
    "    if pd.isna(text):\n",
    "        return True\n",
    "    t = str(text).lower().strip()\n",
    "    garbage = [\"?\", \"??\", \"-\", \".\", \"/\", \"\\\\\", \"–Ω–µ—Ç\", \"–±–ª–æ–∫\", \"—É—Ç\", \"—É—Ç–æ—á\", \"—É—Ç–æ—á–Ω—è—é\", \"http://–Ω–µ—Ç\", \"http://–Ω–µ—Ç \", \"http://-\", \n",
    "               \"http://- \", \"http://.\", \"http://,\", \"http://—É—Ç–æ—á–Ω—è—é —É –±—Ä–µ–Ω–¥–∞\", \"http://—É—Ç–æ—á–Ω—è—é\", \"http://1\", \"http://http:///\", \n",
    "               \"http://http:/// \", \"–ñ–¥—É –∏–Ω—Ñ—É –æ—Ç –±—Ä–µ–Ω–¥–∞\", \"http://–∂–¥—É –∏–Ω—Ñ–æ –æ—Ç –±—Ä–µ–Ω–¥–∞\", \"–∂–¥—É\", \"http://–∂–¥—É\", \n",
    "               \"http://–ñ–¥—É –∏–Ω—Ñ—É –æ—Ç –±—Ä–µ–Ω–¥–∞\", \"http://—É—Ç–æ—á–Ω—è—é —É –±—Ä–µ–Ω–¥–∞ \", \"http://=\", \"http://0\", \" \"]\n",
    "    \n",
    "    return any(t == g or t.startswith(g + \" \") for g in garbage)\n",
    "\n",
    "def unique_join(values):\n",
    "    result = []\n",
    "    for v in values:\n",
    "        if not v:\n",
    "            continue\n",
    "        parts = [p.strip() for p in str(v).split(\"|\")]\n",
    "        for p in parts:\n",
    "            if p and p not in result:\n",
    "                result.append(p)\n",
    "    return \" | \".join(result)\n",
    "\n",
    "# ===== Instagram =====\n",
    "def normalize_instagram_url(url):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        path = parsed.path.strip(\"/\")\n",
    "        if not path:\n",
    "            return \"\"\n",
    "        username = path.split(\"/\")[0]\n",
    "        return f\"https://instagram.com/{username}/\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def clean_instagram(value):\n",
    "    if is_garbage(value):\n",
    "        return \"\"\n",
    "    v = str(value).strip()\n",
    "    v = re.sub(r\"^–±–ª–æ–∫\\s*\", \"\", v, flags=re.IGNORECASE)\n",
    "    tokens = re.split(r\"\\s+\", v)\n",
    "    cleaned = []\n",
    "\n",
    "    for token in tokens:\n",
    "        token = token.strip()\n",
    "        if not token:\n",
    "            continue\n",
    "        if token.startswith(\"#\"):\n",
    "            cleaned.append(token)\n",
    "            continue\n",
    "        if token.startswith(\"@\"):\n",
    "            token = token[1:]\n",
    "        if \"instagram.com\" in token:\n",
    "            norm = normalize_instagram_url(token)\n",
    "            if norm and norm != \"https://www.instagram.com/\":  # —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—É—é\n",
    "                cleaned.append(norm)\n",
    "            continue\n",
    "        if re.match(r\"^[a-zA-Z0-9_.]+$\", token):\n",
    "            cleaned.append(f\"https://instagram.com/{token}/\")\n",
    "\n",
    "    unique = []\n",
    "    for x in cleaned:\n",
    "        if x not in unique:\n",
    "            unique.append(x)\n",
    "    return \" | \".join(unique)\n",
    "\n",
    "# ===== Website =====\n",
    "def normalize_url(url):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        clean = urlunparse((parsed.scheme or \"https\",\n",
    "                            parsed.netloc,\n",
    "                            parsed.path.rstrip(\"/\") + \"/\",\n",
    "                            \"\", \"\", \"\"))\n",
    "        return clean\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "EMPTY_DOMAINS = {\n",
    "    \"www.instagram.com\",\n",
    "    \"instagram.com\",\n",
    "    \"www.ozon.ru\",\n",
    "    \"ozon.ru\"\n",
    "}\n",
    "\n",
    "def clean_site(value):\n",
    "    if is_garbage(value):\n",
    "        return \"\"\n",
    "    v = str(value).strip()\n",
    "\n",
    "    if not v.startswith(\"http\"):\n",
    "        v = \"https://\" + v\n",
    "\n",
    "    norm = normalize_url(v)\n",
    "\n",
    "    # —Ñ–∏–ª—å—Ç—Ä—É–µ–º –±–∏—Ç—ã–µ URL\n",
    "    if norm in [\"http://http/\", \"http:///\"]:\n",
    "        return \"\"\n",
    "\n",
    "    parsed = urlparse(norm)\n",
    "    if not parsed.netloc:\n",
    "        return \"\"\n",
    "\n",
    "    # —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å–∞–π—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞\n",
    "    if parsed.netloc.lower() in EMPTY_DOMAINS:\n",
    "        return \"\"\n",
    "\n",
    "    return norm\n",
    "\n",
    "# ===== VK =====\n",
    "def normalize_vk_url(url):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc.replace(\"m.vk.com\", \"vk.com\")\n",
    "        path = parsed.path.strip(\"/\")\n",
    "        if not path:\n",
    "            return \"\"\n",
    "        return f\"https://vk.com/{path}\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def clean_vk(value):\n",
    "    if is_garbage(value):\n",
    "        return \"\"\n",
    "    v = str(value).strip()\n",
    "    v = re.sub(r\"^–±–ª–æ–∫\\s*\", \"\", v, flags=re.IGNORECASE)\n",
    "    if v.startswith(\"#\"):\n",
    "        v = v[1:]\n",
    "    if \"vk.com\" not in v:\n",
    "        v = f\"https://vk.com/{v.replace('@','')}\"\n",
    "    return normalize_vk_url(v)\n",
    "\n",
    "# ===== Telegram =====\n",
    "def clean_tg(value):\n",
    "    if is_garbage(value):\n",
    "        return \"\"\n",
    "\n",
    "    v = str(value).strip().replace(\"@\", \"\")\n",
    "\n",
    "    # —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å—Å—ã–ª–∫—É\n",
    "    if not v.startswith(\"http\"):\n",
    "        v = f\"https://t.me/{v.split('/')[-1]}\"\n",
    "\n",
    "    if \"t.me\" not in v:\n",
    "        username = v.split(\"/\")[-1]\n",
    "        v = f\"https://t.me/{username}\"\n",
    "\n",
    "    # —É–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Å—ã–ª–∫–∏ —Ç–∏–ø–∞ https://t.me/\n",
    "    if v.rstrip(\"/\") == \"https://t.me\":\n",
    "        return \"\"\n",
    "\n",
    "    return v\n",
    "\n",
    "# ===== –ü—Ä–∏–º–µ–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É =====\n",
    "for col, func in [\n",
    "    (\"Instagram/—Ö—ç—à—Ç–µ–≥\", clean_instagram),\n",
    "    (\"–°—Å—ã–ª–∫–∞ –Ω–∞ —Å–∞–π—Ç\", clean_site),\n",
    "    (\"–í–∫–æ–Ω—Ç–∞–∫—Ç–µ –±—Ä–µ–Ω–¥–∞\", clean_vk),\n",
    "    (\"–¢–µ–ª–µ–≥—Ä–∞–º –±—Ä–µ–Ω–¥–∞\", clean_tg)\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(func)\n",
    "\n",
    "# ===== –ü–µ—Ä–µ–∫–ª–∞–¥—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏ =====\n",
    "def route_links(row):\n",
    "    ig = row.get(\"Instagram/—Ö—ç—à—Ç–µ–≥\", \"\")\n",
    "    site = row.get(\"–°—Å—ã–ª–∫–∞ –Ω–∞ —Å–∞–π—Ç\", \"\")\n",
    "    vk = row.get(\"–í–∫–æ–Ω—Ç–∞–∫—Ç–µ –±—Ä–µ–Ω–¥–∞\", \"\")\n",
    "    tg = row.get(\"–¢–µ–ª–µ–≥—Ä–∞–º –±—Ä–µ–Ω–¥–∞\", \"\")\n",
    "\n",
    "    def classify(link):\n",
    "        if not link:\n",
    "            return None\n",
    "        if \"vk.com\" in link:\n",
    "            return \"vk\"\n",
    "        if \"t.me\" in link:\n",
    "            return \"tg\"\n",
    "        if \"instagram.com\" in link or link.startswith(\"#\"):\n",
    "            return \"ig\"\n",
    "        if link.startswith(\"http\"):\n",
    "            return \"site\"\n",
    "        return \"ig\"\n",
    "\n",
    "    for value in [ig, site, vk, tg]:\n",
    "        target = classify(value)\n",
    "        if target == \"ig\":\n",
    "            ig = unique_join([ig, value])\n",
    "        elif target == \"site\":\n",
    "            site = unique_join([site, value])\n",
    "        elif target == \"vk\":\n",
    "            vk = unique_join([vk, value])\n",
    "        elif target == \"tg\":\n",
    "            tg = unique_join([tg, value])\n",
    "    return pd.Series([ig, site, vk, tg])\n",
    "\n",
    "df[[\n",
    "    \"Instagram/—Ö—ç—à—Ç–µ–≥\",\n",
    "    \"–°—Å—ã–ª–∫–∞ –Ω–∞ —Å–∞–π—Ç\",\n",
    "    \"–í–∫–æ–Ω—Ç–∞–∫—Ç–µ –±—Ä–µ–Ω–¥–∞\",\n",
    "    \"–¢–µ–ª–µ–≥—Ä–∞–º –±—Ä–µ–Ω–¥–∞\"\n",
    "]] = df.apply(route_links, axis=1)\n",
    "\n",
    "# ===== –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –±—Ä–µ–Ω–¥—É =====\n",
    "\n",
    "def merge_sites(values):\n",
    "    urls = []\n",
    "\n",
    "    for v in values:\n",
    "        if not v:\n",
    "            continue\n",
    "        parts = [p.strip() for p in str(v).split(\"|\")]\n",
    "\n",
    "        for p in parts:\n",
    "            norm = normalize_url(p)\n",
    "            if norm:\n",
    "                parsed = urlparse(norm)\n",
    "                netloc = parsed.netloc\n",
    "                # –¥–æ–±–∞–≤–ª—è–µ–º www., –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç\n",
    "                if not netloc.startswith(\"www.\"):\n",
    "                    netloc = \"www.\" + netloc\n",
    "                root_url = f\"{parsed.scheme}://{netloc}/\"\n",
    "                urls.append(root_url)\n",
    "\n",
    "    # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ—Ä–Ω–µ–≤—ã–µ URL\n",
    "    urls = list(dict.fromkeys(urls))\n",
    "    return \" | \".join(urls)\n",
    "\n",
    "\n",
    "grouped = df.groupby(\"–ë—Ä–µ–Ω–¥\").agg({\n",
    "    \"Instagram/—Ö—ç—à—Ç–µ–≥\": unique_join,\n",
    "    \"–°—Å—ã–ª–∫–∞ –Ω–∞ —Å–∞–π—Ç\": merge_sites,\n",
    "    \"–í–∫–æ–Ω—Ç–∞–∫—Ç–µ –±—Ä–µ–Ω–¥–∞\": unique_join,\n",
    "    \"–¢–µ–ª–µ–≥—Ä–∞–º –±—Ä–µ–Ω–¥–∞\": unique_join\n",
    "}).reset_index()\n",
    "\n",
    "# ===== –ü–æ—Å–ª–µ–¥–Ω—è—è —Å–¥–µ–ª–∫–∞ –ø–æ –±—Ä–µ–Ω–¥—É =====\n",
    "df[\"–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è\"] = pd.to_datetime(df.get(\"–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è\"), errors=\"coerce\")\n",
    "\n",
    "# –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ç–∏–º –≤–∑—è—Ç—å\n",
    "required = [\n",
    "    \"–ë—Ä–µ–Ω–¥\", \"–ù–∞–∑–≤–∞–Ω–∏–µ —Å–¥–µ–ª–∫–∏\", \"amoCRM link\", \"–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è\", \"–î–∞—Ç–∞ –∑–∞–∫—Ä—ã—Ç–∏—è\", \"–ò–º—è\", \"–ë–ª–æ–∫ Instagram\", \"–°–µ–≥–º–µ–Ω—Ç\",\n",
    "    \"–¢–∏–ø –±—Ä–µ–Ω–¥–∞\", \"–ò–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç—å –±—Ä–µ–Ω–¥–∞\", \"–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å\",\n",
    "    \"–ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–∞ –¥–ª\"\n",
    "]\n",
    "\n",
    "# –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –≤ df\n",
    "available_cols = [c for c in required if c in df.columns]\n",
    "\n",
    "idx = df.groupby(\"–ë—Ä–µ–Ω–¥\")[\"–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è\"].idxmax()\n",
    "latest_deals = df.loc[idx, available_cols]\n",
    "\n",
    "# ===== –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å grouped =====\n",
    "final = grouped.merge(latest_deals, on=\"–ë—Ä–µ–Ω–¥\", how=\"left\")\n",
    "\n",
    "# ===== –°–æ—Ö—Ä–∞–Ω—è–µ–º =====\n",
    "final.to_excel(OUTPUT_FILE, index=False)\n",
    "print(\"–ì–æ—Ç–æ–≤–æ! –ë–∞–∑–∞ –æ—á–∏—â–µ–Ω–∞ –∏ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å–¥–µ–ª–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞ üéâ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web sites activity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "\n",
    "\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "\n",
    "# ---------- –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–∞–π—Ç–∞ ----------\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "}\n",
    "\n",
    "def site_exists(url):\n",
    "\n",
    "    def try_request(test_url):\n",
    "        try:\n",
    "            r = session.get(\n",
    "                test_url,\n",
    "                headers=HEADERS,\n",
    "                timeout=12,\n",
    "                allow_redirects=True,\n",
    "                verify=False\n",
    "            )\n",
    "\n",
    "            # –µ—Å–ª–∏ —Å–∞–π—Ç –æ—Ç–≤–µ—á–∞–µ—Ç —á–µ–º —É–≥–æ–¥–Ω–æ –∫—Ä–æ–º–µ 5xx ‚Äî —Å—á–∏—Ç–∞–µ–º –∂–∏–≤—ã–º\n",
    "            if r.status_code < 500:\n",
    "                return True, r.url\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return False, test_url\n",
    "\n",
    "    url = str(url).strip()\n",
    "\n",
    "    if not url.startswith(\"http\"):\n",
    "        url = \"https://\" + url\n",
    "\n",
    "    ok, final_url = try_request(url)\n",
    "\n",
    "    if ok:\n",
    "        return True, final_url\n",
    "\n",
    "    http_url = url.replace(\"https://\", \"http://\")\n",
    "    return try_request(http_url)\n",
    "\n",
    "\n",
    "\n",
    "# ---------- –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ----------\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "NEWS_PATHS = [\n",
    "\n",
    "    # --- –±–∞–∑–æ–≤—ã–µ ---\n",
    "    \"/blog\",\n",
    "    \"/news\",\n",
    "    \"/articles\",\n",
    "    \"/press\",\n",
    "    \"/media\",\n",
    "    \"/community\",\n",
    "\n",
    "    # --- —Ä—É—Å—Å–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏ ---\n",
    "    \"/blog/\",\n",
    "    \"/news/\",\n",
    "    \"/novosti\",\n",
    "    \"/novosti/\",\n",
    "    \"/newsroom\",\n",
    "    \"/press-center\",\n",
    "    \"/presscenter\",\n",
    "\n",
    "    # --- —Ä—É—Å—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è ---\n",
    "    \"/–±–ª–æ–≥\",\n",
    "    \"/–Ω–æ–≤–æ—Å—Ç–∏\",\n",
    "    \"/—Å—Ç–∞—Ç—å–∏\",\n",
    "    \"/–ø—Ä–µ—Å—Å\",\n",
    "    \"/—Å–º–∏\",\n",
    "\n",
    "    # --- e-commerce ---\n",
    "    \"/journal\",\n",
    "    \"/stories\",\n",
    "    \"/updates\",\n",
    "    \"/insights\",\n",
    "    \"/magazine\",\n",
    "\n",
    "    # --- –º–∞—Ä–∫–µ—Ç–∏–Ω–≥ / –±—Ä–µ–Ω–¥—ã ---\n",
    "    \"/about/news\",\n",
    "    \"/company/news\",\n",
    "    \"/company/blog\",\n",
    "    \"/academy\",\n",
    "    \"/education\",\n",
    "    \"/events\",\n",
    "\n",
    "    # --- CMS —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ ---\n",
    "    \"/index.php/blog\",\n",
    "    \"/blog.html\",\n",
    "    \"/news.html\"\n",
    "]\n",
    "\n",
    "MONTHS_EN = {\n",
    "    \"january\": 1, \"jan\": 1,\n",
    "    \"february\": 2, \"feb\": 2,\n",
    "    \"march\": 3, \"mar\": 3,\n",
    "    \"april\": 4, \"apr\": 4,\n",
    "    \"may\": 5,\n",
    "    \"june\": 6, \"jun\": 6,\n",
    "    \"july\": 7, \"jul\": 7,\n",
    "    \"august\": 8, \"aug\": 8,\n",
    "    \"september\": 9, \"sep\": 9,\n",
    "    \"october\": 10, \"oct\": 10,\n",
    "    \"november\": 11, \"nov\": 11,\n",
    "    \"december\": 12, \"dec\": 12\n",
    "}\n",
    "\n",
    "MONTHS_RU = {\n",
    "    \"—è–Ω–≤–∞—Ä—è\": 1, \"—Ñ–µ–≤—Ä–∞–ª—è\": 2, \"–º–∞—Ä—Ç–∞\": 3, \"–∞–ø—Ä–µ–ª—è\": 4,\n",
    "    \"–º–∞—è\": 5, \"–∏—é–Ω—è\": 6, \"–∏—é–ª—è\": 7, \"–∞–≤–≥—É—Å—Ç–∞\": 8,\n",
    "    \"—Å–µ–Ω—Ç—è–±—Ä—è\": 9, \"–æ–∫—Ç—è–±—Ä—è\": 10, \"–Ω–æ—è–±—Ä—è\": 11, \"–¥–µ–∫–∞–±—Ä—è\": 12\n",
    "}\n",
    "\n",
    "DATE_PATTERNS = [\n",
    "    r\"\\d{1,2}\\.\\d{1,2}\\.\\d{4}\",  # 23.10.2025\n",
    "    r\"\\d{4}-\\d{2}-\\d{2}\",        # 2025-10-23\n",
    "]\n",
    "\n",
    "RU_PATTERN = re.compile(\n",
    "    r\"(\\d{1,2})\\s+(\" + \"|\".join(MONTHS_RU.keys()) + r\")\\s+(\\d{4})\"\n",
    ")\n",
    "\n",
    "RU_NO_YEAR = re.compile(\n",
    "    r\"(\\d{1,2})\\s+(\" + \"|\".join(MONTHS_RU.keys()) + r\")\"\n",
    ")\n",
    "\n",
    "EN_NO_YEAR = re.compile(\n",
    "    r\"(\\d{1,2})\\s+(\" + \"|\".join(MONTHS_EN.keys()) + r\")\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def guess_year(day, month):\n",
    "    \"\"\" –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–æ–¥, –µ—Å–ª–∏ –æ–Ω –Ω–µ —É–∫–∞–∑–∞–Ω\"\"\"\n",
    "\n",
    "    today = datetime.today()\n",
    "\n",
    "    candidate = datetime(today.year, month, day)\n",
    "\n",
    "    # –µ—Å–ª–∏ –¥–∞—Ç–∞ –≤ –±—É–¥—É—â–µ–º ‚Üí –∑–Ω–∞—á–∏—Ç –ø—Ä–æ—à–ª—ã–π –≥–æ–¥\n",
    "    if candidate > today:\n",
    "        return today.year - 1\n",
    "\n",
    "    return today.year\n",
    "\n",
    "\n",
    "def parse_date(text):\n",
    "\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # dd.mm.yyyy\n",
    "    try:\n",
    "        return datetime.strptime(text, \"%d.%m.%Y\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # yyyy-mm-dd\n",
    "    try:\n",
    "        return datetime.strptime(text, \"%Y-%m-%d\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # —Ä—É—Å—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç –° –ì–û–î–û–ú\n",
    "    match = RU_PATTERN.search(text)\n",
    "    if match:\n",
    "        d, m, y = match.groups()\n",
    "        return datetime(int(y), MONTHS_RU[m], int(d))\n",
    "\n",
    "    # —Ä—É—Å—Å–∫–∏–π –ë–ï–ó –≥–æ–¥–∞\n",
    "    match = RU_NO_YEAR.search(text)\n",
    "    if match:\n",
    "        d, m = match.groups()\n",
    "        year = guess_year(int(d), MONTHS_RU[m])\n",
    "        return datetime(year, MONTHS_RU[m], int(d))\n",
    "\n",
    "    # –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –ë–ï–ó –≥–æ–¥–∞\n",
    "    match = EN_NO_YEAR.search(text)\n",
    "    if match:\n",
    "        d, m = match.groups()\n",
    "        month = MONTHS_EN[m.lower()]\n",
    "        year = guess_year(int(d), month)\n",
    "        return datetime(year, month, int(d))\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_dates_from_page(url):\n",
    "    try:\n",
    "        r = session.get(url, timeout=8, verify=False)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        dates = []\n",
    "\n",
    "        # ---------- –∏—â–µ–º <time datetime=\"...\"> ----------\n",
    "        for t in soup.find_all(\"time\"):\n",
    "            dt_attr = t.get(\"datetime\")\n",
    "            if dt_attr:\n",
    "                dt = parse_date(dt_attr)\n",
    "                if dt:\n",
    "                    dates.append(dt)\n",
    "            else:\n",
    "                text = t.get_text(strip=True)\n",
    "                dt = parse_date(text)\n",
    "                if dt:\n",
    "                    dates.append(dt)\n",
    "\n",
    "        # ---------- –∏—â–µ–º <span class=\"date\"> ----------\n",
    "        for s in soup.find_all(\"span\", class_=\"date\"):\n",
    "            text = s.get_text(strip=True)\n",
    "            dt = parse_date(text)\n",
    "            if dt:\n",
    "                dates.append(dt)\n",
    "\n",
    "        # ---------- fallback: –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã ----------\n",
    "        if not dates:\n",
    "            text = soup.body.get_text(\" \") if soup.body else soup.get_text(\" \")\n",
    "            for pattern in DATE_PATTERNS + [RU_PATTERN, RU_NO_YEAR, EN_NO_YEAR]:\n",
    "                for match in re.findall(pattern, text):\n",
    "                    dt = parse_date(match)\n",
    "                    if dt:\n",
    "                        dates.append(dt)\n",
    "\n",
    "        return max(dates) if dates else None\n",
    "\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_latest_news_date(site):\n",
    "\n",
    "    for path in NEWS_PATHS:\n",
    "        url = site.rstrip(\"/\") + path\n",
    "        dt = extract_dates_from_page(url)\n",
    "\n",
    "        if dt:\n",
    "            return dt.strftime(\"%d.%m.%Y\")\n",
    "\n",
    "    return \"–Ω–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π\"\n",
    "\n",
    "\n",
    "# ---------- –æ–±—Ä–∞–±–æ—Ç–∫–∞ —è—á–µ–π–∫–∏ ----------\n",
    "\n",
    "def check_sites_cell(cell):\n",
    "\n",
    "    # ---------- –ø—É—Å—Ç–∞—è —è—á–µ–π–∫–∞ ----------\n",
    "    if pd.isna(cell) or not str(cell).strip():\n",
    "        return pd.Series({\"status\": \"\", \"news\": \"\"})\n",
    "\n",
    "    parts = [x.strip() for x in str(cell).split(\"|\") if x.strip()]\n",
    "\n",
    "    exists = []\n",
    "    not_exists = []\n",
    "    news_results = []\n",
    "\n",
    "    # ---------- –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–π—Ç–æ–≤ ----------\n",
    "    for site in parts:\n",
    "\n",
    "        ok, normalized = site_exists(site)\n",
    "\n",
    "        # ‚ùå —Å–∞–π—Ç –ù–ï —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Üí —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç—É—Å\n",
    "        if not ok:\n",
    "            not_exists.append(site)\n",
    "            continue  # ‚Üê –∫–ª—é—á–µ–≤–∞—è —Å—Ç—Ä–æ–∫–∞: –Ω–æ–≤–æ—Å—Ç–∏ –ù–ï –∏—â–µ–º\n",
    "\n",
    "        # ‚úÖ —Å–∞–π—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\n",
    "        exists.append(normalized)\n",
    "\n",
    "        date = get_latest_news_date(normalized)\n",
    "\n",
    "        if len(parts) == 1:\n",
    "            news_results.append(date)\n",
    "        else:\n",
    "            news_results.append(f\"{normalized}: {date}\")\n",
    "\n",
    "        time.sleep(random.uniform(1, 2))  # –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø–∞—É–∑–∞\n",
    "\n",
    "    # ---------- —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ----------\n",
    "\n",
    "    # üëâ –æ–¥–∏–Ω —Å–∞–π—Ç\n",
    "    if len(parts) == 1:\n",
    "\n",
    "        if exists:\n",
    "            status = \"–¥–∞\"\n",
    "            news = news_results[0] if news_results else \"\"\n",
    "        else:\n",
    "            status = \"–Ω–µ—Ç\"\n",
    "            news = \"\"\n",
    "\n",
    "    # üëâ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–∞–π—Ç–æ–≤\n",
    "    else:\n",
    "\n",
    "        blocks = []\n",
    "\n",
    "        if exists:\n",
    "            blocks.append(\"–°–∞–π—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: \" + \", \".join(exists))\n",
    "\n",
    "        if not_exists:\n",
    "            blocks.append(\"–°–∞–π—Ç –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: \" + \", \".join(not_exists))\n",
    "\n",
    "        status = \"\\n\".join(blocks)\n",
    "\n",
    "        # –µ—Å–ª–∏ –∂–∏–≤—ã—Ö —Å–∞–π—Ç–æ–≤ –Ω–µ—Ç ‚Üí –Ω–æ–≤–æ—Å—Ç–∏ –ø—É—Å—Ç–æ\n",
    "        news = \" | \".join(news_results) if news_results else \"\"\n",
    "\n",
    "    return pd.Series({\"status\": status, \"news\": news})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"brands_cleaned.xlsx\"\n",
    "df = pd.read_excel(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 6  # –º–æ–∂–Ω–æ 8‚Äì16\n",
    "\n",
    "# ---------- –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞ ----------\n",
    "def process_site(site):\n",
    "\n",
    "    # ---------- 1. –ø—É—Å—Ç–∞—è —è—á–µ–π–∫–∞ ----------\n",
    "    if pd.isna(site) or str(site).strip() == \"\":\n",
    "        return \"\", \"\"\n",
    "\n",
    "    # ---------- 2. —Ä–∞–∑–±–∏–≤–∞–µ–º —Å—Å—ã–ª–∫–∏ ----------\n",
    "    urls = [u.strip() for u in str(site).split(\"|\") if u.strip()]\n",
    "\n",
    "    exists = []\n",
    "    not_exists = []\n",
    "    news_info = []\n",
    "\n",
    "    # ---------- 3. –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π —Å–∞–π—Ç ----------\n",
    "    for url in urls:\n",
    "\n",
    "        ok, normalized = site_exists(url)\n",
    "\n",
    "        if ok:\n",
    "            exists.append(normalized)\n",
    "\n",
    "            date = get_latest_news_date(normalized)\n",
    "\n",
    "            if len(urls) == 1:\n",
    "                # —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞\n",
    "                news_info.append(date)\n",
    "            else:\n",
    "                news_info.append(f\"{normalized}: {date}\")\n",
    "\n",
    "        else:\n",
    "            not_exists.append(url)\n",
    "\n",
    "    # ==================================================\n",
    "    # ---------- 4. –µ—Å–ª–∏ —Å–∞–π—Ç –æ–¥–∏–Ω ----------\n",
    "    # ==================================================\n",
    "\n",
    "    if len(urls) == 1:\n",
    "\n",
    "        if exists:\n",
    "            return \"–¥–∞\", news_info[0]\n",
    "        else:\n",
    "            return \"–Ω–µ—Ç\", \"\"\n",
    "\n",
    "    # ==================================================\n",
    "    # ---------- 5. –µ—Å–ª–∏ —Å–∞–π—Ç–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ ----------\n",
    "    # ==================================================\n",
    "\n",
    "    status_blocks = []\n",
    "\n",
    "    if exists:\n",
    "        status_blocks.append(\n",
    "            \"–°–∞–π—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: \" + \", \".join(exists)\n",
    "        )\n",
    "\n",
    "    if not_exists:\n",
    "        status_blocks.append(\n",
    "            \"–°–∞–π—Ç –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: \" + \", \".join(not_exists)\n",
    "        )\n",
    "\n",
    "    status = \"; \".join(status_blocks)\n",
    "    news = \"; \".join(news_info) if news_info else \"\"\n",
    "\n",
    "    return status, news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = df_test[\"–°—Å—ã–ª–∫–∞ –Ω–∞ —Å–∞–π—Ç\"].tolist()\n",
    "results = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "\n",
    "    for i, result in enumerate(\n",
    "        tqdm(executor.map(process_site, sites), total=len(sites))\n",
    "    ):\n",
    "        results.append(result)\n",
    "\n",
    "        # –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 50 —Å—Ç—Ä–æ–∫\n",
    "        if i % 50 == 0:\n",
    "            tmp = pd.DataFrame(\n",
    "                results,\n",
    "                columns=[\"–°–∞–π—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\", \"–î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –Ω–æ–≤–æ—Å—Ç–∏\"]\n",
    "            )\n",
    "\n",
    "            df_test.loc[:i, [\"–°–∞–π—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\", \"–î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –Ω–æ–≤–æ—Å—Ç–∏\"]] = tmp.values\n",
    "            df_test.to_excel(\"autosave.xlsx\", index=False)\n",
    "\n",
    "# —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –∑–∞–ø–∏—Å—å\n",
    "tmp = pd.DataFrame(results, columns=[\"–°–∞–π—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\", \"–î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –Ω–æ–≤–æ—Å—Ç–∏\"])\n",
    "df_test[[\"–°–∞–π—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\", \"–î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –Ω–æ–≤–æ—Å—Ç–∏\"]] = tmp.values\n",
    "df_test.to_excel(\"brands_sites_news.xlsx\", index=False)\n",
    "\n",
    "print(\"–ì–æ—Ç–æ–≤–æ!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
